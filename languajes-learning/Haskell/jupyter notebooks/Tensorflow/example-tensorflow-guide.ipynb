{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{-# LANGUAGE ScopedTypeVariables #-}\n",
    "--{-# LANGUAGE OverloadedLists #-}\n",
    "\n",
    "import Control.Monad (replicateM_)\n",
    "import qualified Data.Vector as Vector\n",
    "import Data.Vector (Vector(..))\n",
    "\n",
    "import TensorFlow.Core\n",
    "  (Tensor, Value, feed, encodeTensorData, Scalar(..))\n",
    "import TensorFlow.Ops\n",
    "  (add, placeholder, sub, reduceSum, mul)\n",
    "import TensorFlow.GenOps.Core (square)\n",
    "import TensorFlow.Variable (readValue, initializedVariable, Variable)\n",
    "import TensorFlow.Session (runSession, run, runWithFeeds)\n",
    "import TensorFlow.Minimize (gradientDescent, minimizeWith)\n",
    "import TensorFlow.Types (Shape(..))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- We'll take two vectors (of equal size representing the inputs and\n",
    "-- expected outputs of a linear equation.\n",
    "basicExample :: Vector Float -> Vector Float -> IO (Float, Float)\n",
    "basicExample xInput yInput = runSession $ do\n",
    "  -- Everything below this   ^^ takes place in the \"Session\" monad, which we run with \"runSession\".\n",
    "  \n",
    "  -- Get the sizes of out input and expected output\n",
    "  let xSize = fromIntegral $ Vector.length xInput\n",
    "  let ySize = fromIntegral $ Vector.length yInput\n",
    "\n",
    "  -- Make a \"weight\" variable (slope of our line) with initial value 3\n",
    "  (w :: Variable Float) <- initializedVariable 3\n",
    "\n",
    "  -- Make a \"bias\" variable (y-intercept of line) with initial value 1\n",
    "  (b :: Variable Float) <- initializedVariable 1\n",
    "\n",
    "  -- Create \"placeholders\" with the size of our input and output\n",
    "  (x :: Tensor Value Float) <- placeholder $ Shape [xSize]\n",
    "  (y :: Tensor Value Float) <- placeholder $ Shape [ySize]\n",
    "\n",
    "  -- Make our \"model\", which multiplies the weights by the input and\n",
    "  -- then adds the bias. Notice we use \"readValue\" to use operations on\n",
    "  -- variables. This is our \"actual output\" value.\n",
    "  let linear_model = ((readValue w) `mul` x) `add` (readValue b)\n",
    "  \n",
    "  -- Find the difference between actual output and expected output y,\n",
    "  -- and then square it.\n",
    "  let square_deltas = square (linear_model `sub` y)\n",
    "  \n",
    "  -- Get our \"loss\" function by taking the reduced sum of the above,\n",
    "  -- then \"train\" our model by using the gradient descent optimizer.\n",
    "  -- Notice we pass our weights and bias as the parameters that change.\n",
    "  let loss = reduceSum square_deltas\n",
    "  trainStep <- minimizeWith (gradientDescent 0.01) loss [w,b]\n",
    "  \n",
    "  -- \"Train\" our model, but passing our input and output values as\n",
    "  -- \"feeds\" to fill in the placeholder values.\n",
    "  let trainWithFeeds = \\xF yF -> runWithFeeds\n",
    "        [ feed x xF\n",
    "        , feed y yF\n",
    "        ]\n",
    "        trainStep\n",
    "  \n",
    "  -- Run this training step 1000 times. Encode our input as\n",
    "  -- \"TensorData\"\n",
    "  replicateM_ 10000 $\n",
    "    trainWithFeeds\n",
    "      (encodeTensorData (Shape [xSize]) xInput)\n",
    "      (encodeTensorData (Shape [ySize]) yInput)\n",
    "  \n",
    "  -- \"Run\" our variables to see their learned values and return them\n",
    "  (Scalar w_learned, Scalar b_learned) <-\n",
    "    run (readValue w, readValue b)\n",
    "  return (w_learned, b_learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Usage of the \"basicExample\" function. Should result in a tuple like:\n",
    "-- (5, -1)\n",
    "main :: IO ()\n",
    "main = do\n",
    "  results <- basicExample\n",
    "    (Vector.fromList [1.0, 2.0, 3.0, 4.0])\n",
    "    (Vector.fromList [4.0, 9.0, 14.0, 19.0])\n",
    "--    (Vector [1.0, 2.0, 3.0, 4.0]) -- <- this doesn't work because Data.Vector does NOT export Vector data constructor\n",
    "--    (Vector [4.0, 9.0, 14.0, 19.0])\n",
    "\n",
    "  print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.999997,-0.99999267)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "version": "8.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
